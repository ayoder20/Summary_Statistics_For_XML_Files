{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Digital Humanities and Data Science</h2>\n",
    "<h4>Producing and Organizing Content in the Interest of Streamlining Preprocessing</h4>\n",
    "\n",
    "<p>This notebook was written to address the specific issues of one digital humanities project. Messy input data and the tech debt created by variations in the input data were resolved by this notebook in the interest of creating a homogenous data set. This notebook exemplifies how a digital humanist can go on to work with homogeneous data to generate relevent statistics about their data and to group the data by time, place, or characteristics specific to a data point--in our case, one poem. So while there is much in this notebook a digital humanist may find useful, the specific inconsistencies of our project's input data forced this writer to formulate the code below to address inconsistencies that may have existed in only a few data points. The comments in this notebook should help clear up some of the confusion.</p>\n",
    "\n",
    "<p>Some of the specific issues that would interest a Digital Humanist that I've attempted to address in this notebook are:</p>\n",
    "\n",
    "<ul>\n",
    "<li>Formatting XML in the interest of creating good \"input data\" from the standpoint of data analysis--see notes below</li>\n",
    "<li>Creating equalivalencies between data points--E.G. cannot==can not==can't</li>\n",
    "<li>Generating Relavent Statistics about the data--as a whole, grouped by year, etc.</li>\n",
    "</ul>\n",
    "\n",
    "<p>Readers also may be interested in researching the lxml package. lxml extends elementTree (used in this notebook) and supports full XPath capabilities. This writer chose to work with elementTree because it is a standard library and can accomplish the necessary preprocessing.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Notes for Digital Humanists Interested in Examining their XML with Python</h4>\n",
    "\n",
    "<ul>\n",
    "<li>Do not use multiple, identical tags to convey a multiplicative meaning of the tag.\n",
    "e.g. <l rend='indent'><l rend='indent'>twice indented text</l></l>\n",
    "Instead, use a unique tag, e.g. <l rend='indentx2'></l></li>\n",
    "\n",
    "<li>If using a boilerplate XML file to create new content, do not leave empty example tags.</li>\n",
    "\n",
    "<li>Determine at the outset how lingustic equivalencies should be handled. E.G. are Arabic numerals equivalent to spelled numbers</li>\n",
    "\n",
    "<li>Determine at the outset what \"counts\" toward certain statistics. E.G. does the occurrence of 4 count toward a word count</li>\n",
    "\n",
    "<li>A \"flatter\" XML structure is preferable to a deeper one. E.I. An effort to use semantically descriptive XML tags shouldn't really on a series of nested XML tags. Instead, use XML attributes to reduce the number of nested tags. This simplifies XPath expressions. This can go a long way in practice because it can ensure that similar XML content is nested on the same level. For example, in choosing a set of descriptive XML tags, the decision to include a descriptive XML tag would have to be accompanied by another XML tag that represents the negation or absence of the descriptive characteristic of the first tag in order for the content nested within those tags to be on the same level.</li>\n",
    "\n",
    "<li>Artistic license, dialects, slang, and even simple, grammatical expressions represent an intractible problem when attempting to analyze digital humanist texts. While variety is the spice of life, it is an anathema to programmatic problem solving. Apostrophes are an example of how common grammatical forms can confuse a programmatic analysis. Determining whether or not a word is a contraction or a possessive is difficult without additional information. 'the doctor's out' = the doctor is out VS 'the doctor's pen'. A similar example in German would arise in determining whether or not the word 'den deutschen' is in the singular accusative or the pluarl genetive. These considerations lead me to advocate the use or 'regularlizing' tags. The meaning of a specific word or phrase can be clarified by using some variant of the following in the XML: '<reg regular='von den Deutschen'>den Deutschen</reg>'. Then in certain instances, the contents of the regular attribute can be substituted for the contents of the reg tag.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as et\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import scipy.stats as st\n",
    "import matplotlib.pyplot as plt\n",
    "import xlsxwriter\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import statsmodels as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for file in os.listdir(r'/path/to/XML/folder/'):\n",
    "    tree = et.parse(r'/path/to/XML/folder/' + file)\n",
    "    dataDict[file] = tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The below pattern includes the alphanumeric set specific to the German language inlcuding specific punctuation. \n",
    "allCharacters = r'(?i)(?:(?![×Þß÷þø])[a-zÀ-ÿ0-9])'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jukka/anaconda3/lib/python3.6/site-packages/matplotlib/__init__.py:938: UserWarning: axes.color_cycle is deprecated and replaced with axes.prop_cycle; please use the latter.\n",
      "  warnings.warn(self.msg_depr % (key, alt_key))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<scipy.stats._continuous_distns.frechet_l_gen object at 0x7f39669c20f0>\n"
     ]
    }
   ],
   "source": [
    "import xlsxwriter\n",
    "\n",
    "def buildSummary(dictionaryOfDictionaries, allowedRegexExpressions, writePath):\n",
    "    wordList = []\n",
    "    # The below variables will count occurrences of the different kinds\n",
    "    poemCount = 0\n",
    "    wordCount = 0\n",
    "    lineCount = 0\n",
    "    wordCounts = {}\n",
    "    wordsInAPoem = []\n",
    "    poemsDict = {}\n",
    "    for k, v in dataDict.items():\n",
    "        poemDict = {}\n",
    "        poemCount += 1\n",
    "        # Obtain the root element of the XML doc.\n",
    "        root = v.getroot()\n",
    "        wordsInThisPoem = 0\n",
    "        '''\n",
    "        Determine the language of the poem. Note, there may be more than one language present in a poem;\n",
    "        therefore, create a list of languages if there are multiple. Store languages in variable: 'flag'.\n",
    "        This step allows handling of multiple languages.\n",
    "        '''\n",
    "        flag =[]\n",
    "        # When using \".findall\" it's necessary to define the namespace of the document within brackets.\n",
    "        languages = root.findall(\".//{http://www.tei-c.org/ns/1.0}language\")\n",
    "        for language in languages:\n",
    "            if language.text is not None:\n",
    "                flag.append(language.text.lower())\n",
    "                \n",
    "        # Remove annotations. Note, to delete a node, the parent node must be referenced. \n",
    "        note_parents = root.findall(\".//{http://www.tei-c.org/ns/1.0}note/..\")\n",
    "        for parent in note_parents:\n",
    "            # Because an l tag may have more than one note . . .\n",
    "            for note in parent.findall(\".//{http://www.tei-c.org/ns/1.0}note\"):\n",
    "                parent.remove(note)\n",
    "            \n",
    "        # Edit the lines below to locate the particular xml Tag contents in which you are interested\n",
    "        # Here, I want to find \"l\" tags that are children of \"lg\" tags\n",
    "        for lg in root.findall(\".//{http://www.tei-c.org/ns/1.0}lg/{http://www.tei-c.org/ns/1.0}lg\"):\n",
    "            for line in lg.findall(\".//{http://www.tei-c.org/ns/1.0}l\"):\n",
    "                # The below line joins all the text in the sequence they occur, i.e. exhausting all children for a parent\n",
    "                # before proceeding to the next top level parent for all nodes \n",
    "                # returned by .findall. This usage can avoid complicated XPath expressions.                \n",
    "                trueLine = \"\".join([x for x in line.itertext()])\n",
    "                if ('english' in flag) and (trueLine is not None):\n",
    "                    trueLine = re.sub(r\"won't\", \"will not\", trueLine)\n",
    "                    trueLine = re.sub(r\"can\\'t\", \"can not\", trueLine)\n",
    "                    trueLine = re.sub(r\"n\\'t\", \" not\", trueLine)\n",
    "                    trueLine = re.sub(r\"\\'re\", \" are\", trueLine)\n",
    "                    trueLine = re.sub(r\"\\b([a-z]\\w+)'s\\b\", r\"\\1 is\", trueLine)\n",
    "                    trueLine = re.sub(r\"\\'d\", \" would\", trueLine)\n",
    "                    trueLine = re.sub(r\"\\'ll\", \" will\", trueLine)\n",
    "                    trueLine = re.sub(r\"\\'t\", \" not\", trueLine)\n",
    "                    trueLine = re.sub(r\"\\'ve\", \" have\", trueLine)\n",
    "                    trueLine = re.sub(r\"\\'m\", \" am\", trueLine)\n",
    "                    trueLine = re.sub(r\"won’t\", \"will not\", trueLine)\n",
    "                    trueLine = re.sub(r\"can\\’t\", \"can not\", trueLine)\n",
    "                    trueLine = re.sub(r\"n\\’t\", \" not\", trueLine)\n",
    "                    trueLine = re.sub(r\"\\’re\", \" are\", trueLine)\n",
    "                    trueLine = re.sub(r\"\\b([a-z]\\w+)’s\\b\", r\"\\1 is\",trueLine)\n",
    "                    trueLine = re.sub(r\"\\’d\", \" would\", trueLine)\n",
    "                    trueLine = re.sub(r\"\\’ll\", \" will\", trueLine)\n",
    "                    trueLine = re.sub(r\"\\’t\", \" not\", trueLine)\n",
    "                    trueLine = re.sub(r\"\\’ve\", \" have\", trueLine)\n",
    "                    trueLine = re.sub(r\"\\’m\", \" am\", trueLine)\n",
    "                    wordsInLine = trueLine.split()\n",
    "                    lineCount += 1\n",
    "                    # Regularize each letter in a word and discard characters not in allowedRegexExpressions.\n",
    "                    for word in wordsInLine:\n",
    "                        no_punct = ''\n",
    "                        for char in word:\n",
    "                            char = char.lower()\n",
    "                            if re.search(allowedRegexExpressions, char):\n",
    "                                no_punct = no_punct + char\n",
    "                        if no_punct != '':\n",
    "                            wordList.append(no_punct)\n",
    "                            wordsInThisPoem += 1\n",
    "                            wordCount += 1\n",
    "                            poemDict[wordsInThisPoem] = no_punct\n",
    "                elif trueLine is not None:\n",
    "                    wordsInLine = trueLine.split()\n",
    "                    lineCount += 1\n",
    "                    for word in wordsInLine:          \n",
    "                        no_punct = ''\n",
    "                        for char in word:\n",
    "                            char = char.lower()\n",
    "                            if re.search(allCharacters, char):\n",
    "                                no_punct = no_punct + char\n",
    "                        # Check if no_punct is empty\n",
    "                        if no_punct != '':\n",
    "                            wordList.append(no_punct)\n",
    "                            wordsInThisPoem += 1\n",
    "                            wordCount += 1\n",
    "                            poemDict[wordsInThisPoem] = no_punct\n",
    "        # This block of text deals with XML files for which lg tags (stanzas) are not contained in a parent lg tag.\n",
    "        if wordsInThisPoem == 0:\n",
    "            for lg in root.findall(\".//{http://www.tei-c.org/ns/1.0}lg\"):\n",
    "                for line in lg.findall(\".//{http://www.tei-c.org/ns/1.0}l\"):\n",
    "                    trueLine = \"\".join([x for x in line.itertext()])\n",
    "                    if 'english' in flag:\n",
    "                        trueLine = re.sub(r\"won't\", \"will not\", trueLine)\n",
    "                        trueLine = re.sub(r\"can\\'t\", \"can not\", trueLine)\n",
    "                        trueLine = re.sub(r\"n\\'t\", \" not\", trueLine)\n",
    "                        trueLine = re.sub(r\"\\'re\", \" are\", trueLine)\n",
    "                        trueLine = re.sub(r\"\\b([a-z]\\w+)'s\\b\", r\"\\1 is\", trueLine)\n",
    "                        trueLine = re.sub(r\"\\'d\", \" would\", trueLine)\n",
    "                        trueLine = re.sub(r\"\\'ll\", \" will\", trueLine)\n",
    "                        trueLine = re.sub(r\"\\'t\", \" not\", trueLine)\n",
    "                        trueLine = re.sub(r\"\\'ve\", \" have\", trueLine)\n",
    "                        trueLine = re.sub(r\"\\'m\", \" am\", trueLine)\n",
    "                        trueLine = re.sub(r\"won’t\", \"will not\", trueLine)\n",
    "                        trueLine = re.sub(r\"can\\’t\", \"can not\", trueLine)\n",
    "                        trueLine = re.sub(r\"n\\’t\", \" not\", trueLine)\n",
    "                        trueLine = re.sub(r\"\\’re\", \" are\", trueLine)\n",
    "                        trueLine = re.sub(r\"\\b([a-z]\\w+)’s\\b\", r\"\\1 is\",trueLine)\n",
    "                        trueLine = re.sub(r\"\\’d\", \" would\", trueLine)\n",
    "                        trueLine = re.sub(r\"\\’ll\", \" will\", trueLine)\n",
    "                        trueLine = re.sub(r\"\\’t\", \" not\", trueLine)\n",
    "                        trueLine = re.sub(r\"\\’ve\", \" have\", trueLine)\n",
    "                        trueLine = re.sub(r\"\\’m\", \" am\", trueLine)\n",
    "                        wordsInLine = trueLine.split()\n",
    "                        lineCount += 1\n",
    "                        for word in wordsInLine:\n",
    "                            no_punct = ''\n",
    "                            for char in word:\n",
    "                                char = char.lower()\n",
    "                                if re.search(allowedRegexExpressions, char):\n",
    "                                    no_punct = no_punct + char\n",
    "                            if no_punct != '':\n",
    "                                wordList.append(no_punct)\n",
    "                                wordsInThisPoem += 1\n",
    "                                wordCount += 1\n",
    "                                poemDict[wordsInThisPoem] = no_punct\n",
    "                    elif trueLine is not None:\n",
    "                        wordsInLine = trueLine.split()\n",
    "                        lineCount += 1\n",
    "                        for word in wordsInLine:\n",
    "                            wordCount += 1                        \n",
    "                            no_punct = ''\n",
    "                            for char in word:\n",
    "                                char = char.lower()\n",
    "                                if re.search(allCharacters, char):\n",
    "                                    no_punct = no_punct + char\n",
    "                            # Check if no_punct is empty\n",
    "                            if no_punct != '':\n",
    "                                wordList.append(no_punct)\n",
    "                                wordsInThisPoem += 1\n",
    "                                wordCount += 1\n",
    "                                poemDict[wordsInThisPoem] = no_punct\n",
    "        wordsInAPoem.append(wordsInThisPoem)\n",
    "        poemsDict[k] = poemDict                    \n",
    "       \n",
    "    counts = Counter(wordList)\n",
    "    \n",
    "    workbook = xlsxwriter.Workbook(r'{}'.format(writePath))\n",
    "    worksheet1 = workbook.add_worksheet('Summary Statistics')\n",
    "    row = 0\n",
    "    col = 0\n",
    "    worksheet1.write(0, 0, 'Poem Count')\n",
    "    worksheet1.write(0, 1, poemCount)\n",
    "    worksheet1.write(1, 0, 'Word Count')\n",
    "    worksheet1.write(1, 1, wordCount)    \n",
    "    worksheet1.write(2, 0, 'Line Count')\n",
    "    worksheet1.write(2, 1, lineCount)\n",
    "    worksheet1.write(3, 0, 'Unique Words')\n",
    "    worksheet1.write(3, 1, len(set(wordList)))\n",
    "    worksheet1.write(4, 0, 'Average Words Per Line')\n",
    "    worksheet1.write(4, 1, wordCount/lineCount)\n",
    "    worksheet1.write(5, 0, 'Average Words Per Poem')    \n",
    "    worksheet1.write(5, 1, wordCount/poemCount) \n",
    "    \n",
    "    worksheet2 = workbook.add_worksheet('Unique Word Counts')\n",
    "    worksheet2.write(0, 0, 'Word')\n",
    "    worksheet2.write(0, 1, 'Count')\n",
    "    row = 0\n",
    "    col = 0\n",
    "    for key in counts.keys():\n",
    "        row += 1\n",
    "        worksheet2.write(row, col, key)\n",
    "        item = counts[key]\n",
    "        worksheet2.write(row, col + 1, item)\n",
    "    plt.rcParams['figure.figsize'] = (16.0, 12.0)\n",
    "    plt.style.use('ggplot')\n",
    "    \n",
    "    data = pd.Series(wordsInAPoem)\n",
    "    # Plot for comparison\n",
    "    plt.figure(figsize=(12,8))\n",
    "    ax = data.plot(kind='hist', bins=200, normed=True, alpha=0.5, color=plt.rcParams['axes.color_cycle'][1])\n",
    "    # Save plot limits\n",
    "    dataYLim = ax.get_ylim()\n",
    "\n",
    "    # Find best fit distribution\n",
    "    best_fit_name, best_fir_paramms = best_fit_distribution(data, 200, ax)\n",
    "    best_dist = getattr(st, best_fit_name)\n",
    "\n",
    "    print(best_dist)\n",
    "    # Update plots\n",
    "    ax.set_ylim(dataYLim)\n",
    "    ax.set_title('Poem Word Count. Best Distribution: ')\n",
    "    ax.set_xlabel('Word Count')\n",
    "    ax.set_ylabel('Frequency')\n",
    "    plt.savefig('allDistributions.png', dpi=150)\n",
    "    # Make PDF\n",
    "    pdf = make_pdf(best_dist, best_fir_paramms)\n",
    "\n",
    "    # Display\n",
    "    plt.figure(figsize=(12,8))\n",
    "    ax = pdf.plot(lw=2, label='PDF', legend=True)\n",
    "    data.plot(kind='hist', bins=200, normed=True, alpha=0.5, label='Data', legend=True, ax=ax)\n",
    "\n",
    "    param_names = (best_dist.shapes + ', loc, scale').split(', ') if best_dist.shapes else ['loc', 'scale']\n",
    "    param_str = ', '.join(['{}={:0.2f}'.format(k,v) for k,v in zip(param_names, best_fir_paramms)])\n",
    "    dist_str = '{}({})'.format(best_fit_name, param_str)\n",
    "\n",
    "    ax.set_title('Poem Word Count. Best Distribution:  \\n' + dist_str)\n",
    "    ax.set_xlabel('Word Count')\n",
    "    ax.set_ylabel('Frequency')\n",
    "    \n",
    "    worksheet3 = workbook.add_worksheet('Fitted Distributions')\n",
    "    worksheet3.insert_image('A1', 'bestDistribution.png')\n",
    "    #worksheet3.insert_image('A36', 'allDistributions.png')\n",
    "    \n",
    "    workbook.close()\n",
    "    \n",
    "    return  wordsInAPoem, poemsDict, wordList\n",
    "    \n",
    "wordsInAPoem, poemsDict, wordList = buildSummary(dataDict, allCharacters, r'test.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create models from data\n",
    "def best_fit_distribution(data, bins=200, ax=None):\n",
    "    \"\"\"Model data by finding best fit distribution to data\"\"\"\n",
    "    # Get histogram of original data\n",
    "    y, x = np.histogram(data, bins=bins, density=True)\n",
    "    x = (x + np.roll(x, -1))[:-1] / 2.0\n",
    "\n",
    "    # Distributions to check\n",
    "    DISTRIBUTIONS = [        \n",
    "        st.alpha,st.anglit,st.arcsine,st.beta,st.betaprime,st.bradford,st.burr,st.cauchy,st.chi,st.chi2,st.cosine,\n",
    "        st.dgamma,st.dweibull,st.erlang,st.expon,st.exponnorm,st.exponweib,st.exponpow,st.f,st.fatiguelife,st.fisk,\n",
    "        st.foldcauchy,st.foldnorm,st.frechet_r,st.frechet_l,st.genlogistic,st.genpareto,st.gennorm,st.genexpon,\n",
    "        st.genextreme,st.gausshyper,st.gamma,st.gengamma,st.genhalflogistic,st.gilbrat,st.gompertz,st.gumbel_r,\n",
    "        st.gumbel_l,st.halfcauchy,st.halflogistic,st.halfnorm,st.halfgennorm,st.hypsecant,st.invgamma,st.invgauss,\n",
    "        st.invweibull,st.johnsonsb,st.johnsonsu,st.ksone,st.kstwobign,st.laplace,st.levy,st.levy_l,st.levy_stable,\n",
    "        st.logistic,st.loggamma,st.loglaplace,st.lognorm,st.lomax,st.maxwell,st.mielke,st.nakagami,st.ncx2,st.ncf,\n",
    "        st.nct,st.norm,st.pareto,st.pearson3,st.powerlaw,st.powerlognorm,st.powernorm,st.rdist,st.reciprocal,\n",
    "        st.rayleigh,st.rice,st.recipinvgauss,st.semicircular,st.t,st.triang,st.truncexpon,st.truncnorm,st.tukeylambda,\n",
    "        st.uniform,st.vonmises,st.vonmises_line,st.wald,st.weibull_min,st.weibull_max,st.wrapcauchy\n",
    "    ]\n",
    "\n",
    "    # Best holders\n",
    "    best_distribution = st.norm\n",
    "    best_params = (0.0, 1.0)\n",
    "    best_sse = np.inf\n",
    "\n",
    "    # Estimate distribution parameters from data\n",
    "    for distribution in DISTRIBUTIONS:\n",
    "\n",
    "        # Try to fit the distribution\n",
    "        try:\n",
    "            # Ignore warnings from data that can't be fit\n",
    "            with warnings.catch_warnings():\n",
    "                warnings.filterwarnings('ignore')\n",
    "\n",
    "                # fit dist to data\n",
    "                params = distribution.fit(data)\n",
    "\n",
    "                # Separate parts of parameters\n",
    "                arg = params[:-2]\n",
    "                loc = params[-2]\n",
    "                scale = params[-1]\n",
    "\n",
    "                # Calculate fitted PDF and error with fit in distribution\n",
    "                pdf = distribution.pdf(x, loc=loc, scale=scale, *arg)\n",
    "                sse = np.sum(np.power(y - pdf, 2.0))\n",
    "\n",
    "                # if axis pass in add to plot\n",
    "                try:\n",
    "                    if ax:\n",
    "                        pd.Series(pdf, x).plot(ax=ax)\n",
    "                    end\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "                # identify if this distribution is better\n",
    "                if best_sse > sse > 0:\n",
    "                    best_distribution = distribution\n",
    "                    best_params = params\n",
    "                    best_sse = sse\n",
    "\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    return (best_distribution.name, best_params)\n",
    "\n",
    "def make_pdf(dist, params, size=10000):\n",
    "    \"\"\"Generate distributions's Propbability Distribution Function \"\"\"\n",
    "\n",
    "    # Separate parts of parameters\n",
    "    arg = params[:-2]\n",
    "    loc = params[-2]\n",
    "    scale = params[-1]\n",
    "\n",
    "    # Get sane start and end points of distribution\n",
    "    start = dist.ppf(0.01, *arg, loc=loc, scale=scale) if arg else dist.ppf(0.01, loc=loc, scale=scale)\n",
    "    end = dist.ppf(0.99, *arg, loc=loc, scale=scale) if arg else dist.ppf(0.99, loc=loc, scale=scale)\n",
    "\n",
    "    # Build PDF and turn into pandas Series\n",
    "    x = np.linspace(start, end, size)\n",
    "    y = dist.pdf(x, loc=loc, scale=scale, *arg)\n",
    "    pdf = pd.Series(y, x)\n",
    "\n",
    "    return pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
